{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d1724f",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46240d43",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "- Write modular code with relevant docstrings and comments for you to be able to use\n",
    "functions you have implemented in future assignments.\n",
    "- All theory questions and observations must be written in a markdown cell of your jupyter notebook.You can alsoadd necessary images in `imgs/` and then include it in markdown. Any other submission method for theoretical question won't be entertained.\n",
    "- Start the assignment early, push your code regularly and enjoy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d6a65",
   "metadata": {},
   "source": [
    "### Question 1 Optimal DT from table\n",
    "**[20 points]**\\\n",
    "We will use the dataset below to learn a decision tree which predicts if people pass machine\n",
    "learning (Yes or No), based on their previous GPA (High, Medium, or Low) and whether or\n",
    "not they studied. \n",
    "\n",
    "| GPA | Studied | Passed |\n",
    "|:---:|:-------:|:------:|\n",
    "|  L  |    F    |    F   |\n",
    "|  L  |    T    |    T   |\n",
    "|  M  |    F    |    F   |\n",
    "|  M  |    T    |    T   |\n",
    "|  H  |    F    |    T   |\n",
    "|  H  |    T    |    T   |\n",
    "    \n",
    " For this problem, you can write your answers using $log_2$\n",
    ", but it may be helpful to note\n",
    "that $log_2 3 â‰ˆ 1.6$.\n",
    "\n",
    "---\n",
    "1. What is the entropy H(Passed)?\n",
    "2. What is the entropy H(Passed | GPA)?\n",
    "3. What is the entropy H(Passed | Studied)?\n",
    "4. Draw the full decision tree that would be learned for this dataset. You do\n",
    "not need to show any calculations.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cf408",
   "metadata": {},
   "source": [
    "### Question 2 DT loss functions\n",
    "**[10 points]**\n",
    "1. Explain Gini impurity and Entropy. \n",
    "2. What are the min and max values for both Gini impurity and Entropy\n",
    "3. Plot the Gini impurity and Entropy for $p\\in[0,1]$.\n",
    "4. Multiply Gini impurity by a factor of 2 and overlay it over entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a406ac",
   "metadata": {},
   "source": [
    "### Question 3 Training a Decision Tree  \n",
    "**[40 points]**\n",
    "\n",
    "You can download the spam dataset from the link given below. This dataset contains feature vectors and the lables of Spam/Non-Spam mails. \n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "\n",
    "**NOTE: The last column in each row represents whether the mail is spam or non spam**\\\n",
    "Although not needed, incase you want to know what the individual columns in the feature vector means, you can read it in the documentation given below.\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de757a",
   "metadata": {},
   "source": [
    "**Download the data and load it from the code given below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "163c0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "download_url = (\"./spambase.data\")\n",
    "cols = np.arange(0,58,1)\n",
    "df = pd.read_csv(download_url,names=cols)\n",
    "pd.set_option(\"display.max.rows\", None)\n",
    "data = pd.DataFrame(df).to_numpy()\n",
    "num = len(data)\n",
    "feat = len(data[0])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4227b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "X = data[:,0:feat]\n",
    "label = data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffee80e",
   "metadata": {},
   "source": [
    "You can try to normalize each column (feature) separately with wither one of the following ideas. **Do not normalize labels**.\n",
    "- Shift-and-scale normalization: substract the minimum, then divide by new maximum. Now all values are between 0-1\n",
    "- Zero mean, unit variance : substract the mean, divide by the appropriate value to get variance=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "e67b0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - X.mean(axis=0)\n",
    "X=X/X.std(axis=0)\n",
    "no_of_features = 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef858082",
   "metadata": {},
   "source": [
    "1. Split your data into train 80% and test dataset 20% \n",
    "2. **[BONUS]** Visualize the data using PCA . You can reduce the dimension of the data if you want. Bonus marks if this increases your accuracy.\n",
    "\n",
    "*NOTE: If you are applying PCA or any other type of dimensionality reduction, do it before splitting the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c6b71a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 44)\n",
      "[1, 2, 3, 4, 6, 7, 9, 11, 12, 16, 17, 18, 19, 20, 21, 23, 31, 32, 37, 38, 40, 41, 42, 45, 46, 47, 51, 53, 55, 56]\n",
      "(4601, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Y=X\n",
    "principal=PCA(n_components=44)\n",
    "principal.fit(Y)\n",
    "Y=principal.transform(Y)\n",
    "print(Y.shape)\n",
    "n_pcs= principal.components_.shape[0]\n",
    "most_important = [np.abs(principal.components_[i]).argmax() for i in range(n_pcs)]\n",
    "most_important.sort()\n",
    "ind = list(set(most_important))\n",
    "print(ind)\n",
    "j = np.arange(0,10,1)\n",
    "X = X[:,j]\n",
    "no_of_features = X.shape[1]\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c580af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 57)\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "n_pcs= principal.components_.shape[0]\n",
    "print(principal.components_.shape)\n",
    "print(n_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "2ac96f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "3eb8fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6bf66",
   "metadata": {},
   "source": [
    "You need to perform a K fold validation on this and report the average training error over all the k validations. \n",
    "- For this , you need to split the training data into k splits.\n",
    "- For each split, train a decision tree model and report the training , validation and test scores.\n",
    "- Report the scores in a tabular form for each validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "604495ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "x_data = np.array(np.array_split(X_train,K))\n",
    "y_data = np.array(np.array_split(y_train,K))\n",
    "tr_ac = []\n",
    "test_ac = []\n",
    "val_ac = []\n",
    "for i in range(10):\n",
    "    ind = np.setxor1d(np.arange(0,10),i)\n",
    "    x_trainset = x_data[ind].reshape((-1,no_of_features))\n",
    "    y_trainset = y_data[ind].reshape((-1,1))\n",
    "    x_valset = x_data[i]\n",
    "    y_valset = y_data[i]\n",
    "    model = DecisionTreeClassifier()\n",
    "    model = model.fit(x_trainset,y_trainset)\n",
    "    y_predval = model.predict(x_valset)\n",
    "    y_predtest = model.predict(X_test)\n",
    "    y_predtrain = model.predict(X_train)\n",
    "    tr_ac.append(metrics.accuracy_score(y_train,y_predtrain))\n",
    "    val_ac.append(metrics.accuracy_score(y_valset,y_predval))\n",
    "    test_ac.append(metrics.accuracy_score(y_test,y_predtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "47c82998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   split_number  Training Accuracy  Validation Accuracy  Testing Accuracy\n",
      "0             0           0.938587             0.820652          0.820847\n",
      "1             1           0.938043             0.831522          0.818675\n",
      "2             2           0.938587             0.793478          0.824104\n",
      "3             3           0.938587             0.831522          0.818675\n",
      "4             4           0.938315             0.823370          0.838219\n",
      "5             5           0.939674             0.820652          0.830619\n",
      "6             6           0.941848             0.845109          0.828447\n",
      "7             7           0.938859             0.820652          0.838219\n",
      "8             8           0.940761             0.839674          0.826276\n",
      "9             9           0.941033             0.847826          0.813246\n"
     ]
    }
   ],
   "source": [
    "accuracies = pd.DataFrame({\"split_number\":np.arange(0,10,1),\"Training Accuracy\":tr_ac, \"Validation Accuracy\":val_ac,\"Testing Accuracy\":test_ac})\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcdf68",
   "metadata": {},
   "source": [
    "### Question 4 Random Forest Algorithm\n",
    "**[30 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61115eaf",
   "metadata": {},
   "source": [
    "1. What is boosting, bagging and  stacking?\n",
    "Which class does random forests belong to and why? **[5 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c366d",
   "metadata": {},
   "source": [
    "2. Implement random forest algorithm using different decision trees. **[25 points]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "dd593709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_algorithm(X_train,y_train,X_test,y_test):\n",
    "    tot_trees = 100\n",
    "    rand_feats = 30\n",
    "    rand_data = int(66*X_train.shape[0]/100)\n",
    "    pred_trees = np.zeros((tot_trees,X_test.shape[0]))\n",
    "    for i in range(tot_trees):\n",
    "        feat_ind = random.sample(list(np.arange(0,feat,1)),rand_feats)\n",
    "        feat_ind.sort()\n",
    "        data_ind = random.sample(list(np.arange(0,X_train.shape[0],1)),rand_data)\n",
    "        data_ind.sort()\n",
    "        data = X_train[data_ind][:,feat_ind]\n",
    "        labels = y_train[data_ind]\n",
    "        model = DecisionTreeClassifier()\n",
    "        model = model.fit(data,labels)\n",
    "        pred_trees[i] = model.predict(X_test[:,feat_ind])\n",
    "    # Testing\n",
    "    pred = np.zeros(X_test.shape[0])\n",
    "    sums = pred_trees.sum(axis=0)\n",
    "    for i in range(len(sums)):\n",
    "        if(((sums[i]-(tot_trees/2))>=0)):\n",
    "            pred[i]=1\n",
    "        else:\n",
    "            pred[i]=0\n",
    "    print(\"The test accuracy obtained with random forests is \",metrics.accuracy_score(y_test,pred))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6b8beeac",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [328]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrandom_forest_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [327]\u001b[0m, in \u001b[0;36mrandom_forest_algorithm\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m data_ind \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m)),rand_data)\n\u001b[1;32m     10\u001b[0m data_ind\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m---> 11\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_ind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeat_ind\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m labels \u001b[38;5;241m=\u001b[39m y_train[data_ind]\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "random_forest_algorithm(X_train,y_train,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

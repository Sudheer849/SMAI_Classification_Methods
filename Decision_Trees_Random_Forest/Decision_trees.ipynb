{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d1724f",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46240d43",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "- Write modular code with relevant docstrings and comments for you to be able to use\n",
    "functions you have implemented in future assignments.\n",
    "- All theory questions and observations must be written in a markdown cell of your jupyter notebook.You can alsoadd necessary images in `imgs/` and then include it in markdown. Any other submission method for theoretical question won't be entertained.\n",
    "- Start the assignment early, push your code regularly and enjoy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d6a65",
   "metadata": {},
   "source": [
    "### Question 1 Optimal DT from table\n",
    "**[20 points]**\\\n",
    "We will use the dataset below to learn a decision tree which predicts if people pass machine\n",
    "learning (Yes or No), based on their previous GPA (High, Medium, or Low) and whether or\n",
    "not they studied. \n",
    "\n",
    "| GPA | Studied | Passed |\n",
    "|:---:|:-------:|:------:|\n",
    "|  L  |    F    |    F   |\n",
    "|  L  |    T    |    T   |\n",
    "|  M  |    F    |    F   |\n",
    "|  M  |    T    |    T   |\n",
    "|  H  |    F    |    T   |\n",
    "|  H  |    T    |    T   |\n",
    "    \n",
    " For this problem, you can write your answers using $log_2$\n",
    ", but it may be helpful to note\n",
    "that $log_2 3 â‰ˆ 1.6$.\n",
    "\n",
    "---\n",
    "1. What is the entropy H(Passed)?\n",
    "2. What is the entropy H(Passed | GPA)?\n",
    "3. What is the entropy H(Passed | Studied)?\n",
    "4. Draw the full decision tree that would be learned for this dataset. You do\n",
    "not need to show any calculations.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e415d73",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "1. H(Passed) = -$P(T).\\log(P(T)) - P(F).\\log(P(F))$\n",
    "    \n",
    "   $ = -\\frac{4}{6}.\\log(\\frac{4}{6}) - \\frac{2}{6}.\\log(\\frac{2}{6})$\n",
    "   \n",
    "   $ = -\\frac{2}{3}.\\log(\\frac{2}{3}) - \\frac{1}{3}.\\log(\\frac{1}{3})$\n",
    "   \n",
    "   $ = -\\frac{2}{3}.(1 - log_2 3) - \\frac{1}{3}.(-log_2 3) $\n",
    "   \n",
    "   $ = -\\frac{2}{3}.(-0.6) + \\frac{1}{3}.(1.6) $\n",
    "   \n",
    "   $ = 0.9333 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e5280d",
   "metadata": {},
   "source": [
    "2. H(Passed | GPA) = -$P(L).P(T|L).\\log(P(T|L)) - P(L).P(F|L).\\log(P(F|L)) - P(M).P(T|M).\\log(P(T|M)) - P(M).P(F|M).\\log(P(F|M)) - P(H).P(T|H).\\log(P(T|H)) - P(H).P(F|H).\\log(P(F|H))$\n",
    "    \n",
    "   $ = - \\frac{1}{6}.\\log(\\frac{1}{2}) - \\frac{1}{6}.\\log(\\frac{1}{2}) - \\frac{1}{6}.\\log(\\frac{1}{2}) - \\frac{1}{6}.\\log(\\frac{1}{2}) - \\frac{1}{1}.\\log(\\frac{1}{1}) - 0)$\n",
    "   \n",
    "   $ = 0.667 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ee4e9",
   "metadata": {},
   "source": [
    "2. H(Passed | Studied) = -$P(T).P(T|T).\\log(P(T|T)) - P(T).P(F|T).\\log(P(F|T)) - P(F).P(T|F).\\log(P(T|F)) - P(F).P(F|F).\\log(P(F|F)) $\n",
    "   $ = - \\frac{1}{2}.\\log(1) - 0 - \\frac{1}{6}.\\log(\\frac{1}{3}) - \\frac{1}{3}.\\log(\\frac{2}{3})$\n",
    "   \n",
    "   $ = 0.4667 $\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bdab8",
   "metadata": {},
   "source": [
    "   ![Alt text](Imgs/Decisiontree.png \"a title\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2927193d",
   "metadata": {},
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cf408",
   "metadata": {},
   "source": [
    "### Question 2 DT loss functions\n",
    "**[10 points]**\n",
    "1. Explain Gini impurity and Entropy. \n",
    "2. What are the min and max values for both Gini impurity and Entropy\n",
    "3. Plot the Gini impurity and Entropy for $p\\in[0,1]$.\n",
    "4. Multiply Gini impurity by a factor of 2 and overlay it over entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b5bae",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "#### Gini impurity\n",
    "\n",
    "* Gini impurity can be given as \n",
    " \n",
    " $ \\sum_{_i\\neq j} P({w_i}).P({w_j})$\n",
    "\n",
    " This is a kind of impurity from which we can calculate information gain. This can be used to find the best split\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "* Entropy can be given as \n",
    " \n",
    " $ \\sum_{_i} P(i).\\log(P(i))$\n",
    "\n",
    " This is a kind of impurity from which we can calculate information gain. This can be used to find the best split\n",
    "* Computationally gini impurity is better than Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fad7b",
   "metadata": {},
   "source": [
    "### Q2\n",
    "* Maximum value of Gini Impurity : 0.5\n",
    "* Minimum value of Gini Impurity : 0\n",
    "* Maximum value of Entropy : 1\n",
    "* Minimum value of Entropy : 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ac54e",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    " ![Alt text](Imgs/Entropy.png \"a title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568e0b5",
   "metadata": {},
   "source": [
    "### Gini Impurity\n",
    "\n",
    " ![Alt text](Imgs/GiniEntropy.png \"a title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ad0f1",
   "metadata": {},
   "source": [
    "### Q4\n",
    " ![Alt text](Imgs/overlap.png \"a title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a406ac",
   "metadata": {},
   "source": [
    "### Question 3 Training a Decision Tree  \n",
    "**[40 points]**\n",
    "\n",
    "You can download the spam dataset from the link given below. This dataset contains feature vectors and the lables of Spam/Non-Spam mails. \n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "\n",
    "**NOTE: The last column in each row represents whether the mail is spam or non spam**\\\n",
    "Although not needed, incase you want to know what the individual columns in the feature vector means, you can read it in the documentation given below.\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de757a",
   "metadata": {},
   "source": [
    "**Download the data and load it from the code given below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "163c0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "download_url = (\"./spambase.data\")\n",
    "cols = np.arange(0,58,1)\n",
    "df = pd.read_csv(download_url,names=cols)\n",
    "pd.set_option(\"display.max.rows\", None)\n",
    "data = pd.DataFrame(df).to_numpy()\n",
    "num = len(data)\n",
    "feat = len(data[0])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4227b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "X = data[:,0:feat]\n",
    "label = data[:,-1]\n",
    "Xcopy = X\n",
    "labelcopy = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffee80e",
   "metadata": {},
   "source": [
    "You can try to normalize each column (feature) separately with wither one of the following ideas. **Do not normalize labels**.\n",
    "- Shift-and-scale normalization: substract the minimum, then divide by new maximum. Now all values are between 0-1\n",
    "- Zero mean, unit variance : substract the mean, divide by the appropriate value to get variance=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e67b0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - X.mean(axis=0)\n",
    "X=X/X.std(axis=0)\n",
    "no_of_features = 57\n",
    "pca = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb9ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef858082",
   "metadata": {},
   "source": [
    "1. Split your data into train 80% and test dataset 20% \n",
    "2. **[BONUS]** Visualize the data using PCA . You can reduce the dimension of the data if you want. Bonus marks if this increases your accuracy.\n",
    "\n",
    "*NOTE: If you are applying PCA or any other type of dimensionality reduction, do it before splitting the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c6b71a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca==1:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    Y=X\n",
    "    principal=PCA(n_components=30)\n",
    "    principal.fit(Y)\n",
    "    Y=principal.transform(Y)\n",
    "    n_pcs= principal.components_.shape[0]\n",
    "    most_important = [np.abs(principal.components_[i]).argmax() for i in range(n_pcs)]\n",
    "    most_important.sort()\n",
    "    ind = list(set(most_important))\n",
    "    print(ind)\n",
    "    X = X[:,ind]\n",
    "    no_of_features = X.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2ac96f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3eb8fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 57)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6bf66",
   "metadata": {},
   "source": [
    "You need to perform a K fold validation on this and report the average training error over all the k validations. \n",
    "- For this , you need to split the training data into k splits.\n",
    "- For each split, train a decision tree model and report the training , validation and test scores.\n",
    "- Report the scores in a tabular form for each validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "604495ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "x_data = np.array(np.array_split(X_train,K))\n",
    "y_data = np.array(np.array_split(y_train,K))\n",
    "tr_ac = []\n",
    "test_ac = []\n",
    "val_ac = []\n",
    "for i in range(10):\n",
    "    ind = np.setxor1d(np.arange(0,10),i)\n",
    "    x_trainset = x_data[ind].reshape((-1,no_of_features))\n",
    "    y_trainset = y_data[ind].reshape((-1,1))\n",
    "    x_valset = x_data[i]\n",
    "    y_valset = y_data[i]\n",
    "    model = DecisionTreeClassifier()\n",
    "    model = model.fit(x_trainset,y_trainset)\n",
    "    y_predval = model.predict(x_valset)\n",
    "    y_predtest = model.predict(X_test)\n",
    "    y_predtrain = model.predict(X_train)\n",
    "    tr_ac.append(metrics.accuracy_score(y_train,y_predtrain))\n",
    "    val_ac.append(metrics.accuracy_score(y_valset,y_predval))\n",
    "    test_ac.append(metrics.accuracy_score(y_test,y_predtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "47c82998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   split_number  Training Accuracy  Validation Accuracy  Testing Accuracy\n",
      "0             0           0.991576             0.918478          0.927253\n",
      "1             1           0.994022             0.945652          0.910966\n",
      "2             2           0.989946             0.904891          0.908795\n",
      "3             3           0.991304             0.918478          0.908795\n",
      "4             4           0.992391             0.929348          0.918567\n",
      "5             5           0.987772             0.880435          0.918567\n",
      "6             6           0.990489             0.907609          0.906623\n",
      "7             7           0.991033             0.915761          0.915309\n",
      "8             8           0.991033             0.915761          0.912052\n",
      "9             9           0.992391             0.926630          0.914224\n"
     ]
    }
   ],
   "source": [
    "accuracies = pd.DataFrame({\"split_number\":np.arange(0,10,1),\"Training Accuracy\":tr_ac, \"Validation Accuracy\":val_ac,\"Testing Accuracy\":test_ac})\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcdf68",
   "metadata": {},
   "source": [
    "### Question 4 Random Forest Algorithm\n",
    "**[30 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61115eaf",
   "metadata": {},
   "source": [
    "1. What is boosting, bagging and  stacking?\n",
    "Which class does random forests belong to and why? **[5 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d8d95",
   "metadata": {},
   "source": [
    "### 1\n",
    "* Boosting : Boosting is a technique which combines many weak classifiers to form a strong classifier. Each weak model tries to compensate the weakness of previous weak model. Every weak classifier tries to estimate weak rules which combines to form strong rules.All the individual models are trained sequencially.\n",
    "* Bagging : In bagging , individual models are trained seperately.For each individual model, the model is trained on the data which is taken randomly from the whole data. This helps in reducing overfitting as it reduces the varaince. All the individual models can be trained parellely\n",
    "* Stacking : Stacking is a ensemble technique in which every individual model has some weight associated with it. As we are considering weights the whole model is able to predict well.This is a parellel technique. All the individual models can be trained parellely\n",
    "* Random Forest make use of bagging in which each individual model trains a decision tree. Random features are also selected from the data for training each individual tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c366d",
   "metadata": {},
   "source": [
    "2. Implement random forest algorithm using different decision trees. **[25 points]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd593709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_algorithm(X_train,y_train,X_test,y_test):\n",
    "    tot_trees = 100\n",
    "    rand_feats = 20\n",
    "    rand_data = int(66*X_train.shape[0]/100)\n",
    "    pred_trees = np.zeros((tot_trees,X_test.shape[0]))\n",
    "    for i in range(tot_trees):\n",
    "        feat_ind = random.sample(list(np.arange(0,feat,1)),rand_feats)\n",
    "        feat_ind.sort()\n",
    "        data_ind = random.sample(list(np.arange(0,X_train.shape[0],1)),rand_data)\n",
    "        data_ind.sort()\n",
    "        data = X_train[data_ind][:,feat_ind]\n",
    "        labels = y_train[data_ind]\n",
    "        model = DecisionTreeClassifier()\n",
    "        model = model.fit(data,labels)\n",
    "        pred_trees[i] = model.predict(X_test[:,feat_ind])\n",
    "    # Testing\n",
    "    pred = np.zeros(X_test.shape[0])\n",
    "    sums = pred_trees.sum(axis=0)\n",
    "    for i in range(len(sums)):\n",
    "        if(((sums[i]-(tot_trees/2))>=0)):\n",
    "            pred[i]=1\n",
    "        else:\n",
    "            pred[i]=0\n",
    "    print(\"The test accuracy obtained with random forests is \",metrics.accuracy_score(y_test,pred))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6b8beeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy obtained with random forests is  0.9478827361563518\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xcopy, labelcopy, test_size=0.2, random_state=42) \n",
    "random_forest_algorithm(X_train,y_train,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
